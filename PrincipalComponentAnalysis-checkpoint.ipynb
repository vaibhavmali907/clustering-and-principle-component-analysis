{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5h5s6gTW579"
   },
   "source": [
    "# DIMENSIONALITY REDUCTION USING PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zd6-sZsW58E"
   },
   "source": [
    "<a id=section1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taCbogXRW58G"
   },
   "source": [
    "## 1. Introduction to Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfnonKSVW58J"
   },
   "source": [
    "<a id=section101></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FkCbDx3OW58L"
   },
   "source": [
    "### 1.1 What is Data Dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rX3X-3OcW58N"
   },
   "source": [
    "In real world, __number of columns__ is the number of __dimensions of data__.However, some columns are __similar__, some are __correlated__, some are __duplicates__ in some way, some are __junk__, some are __useless__, etc. so the actual number of dimensions can be unknown. Its a knotty problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MQ6SksjW58P"
   },
   "source": [
    "<a id=section102></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xD2_Aod2W58Q"
   },
   "source": [
    "### 1.2 What is high dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X33fPaAnW58U"
   },
   "source": [
    "Suppose we have __500 variables__ in a data set. As it a very huge number, so it is quite difficult to read and understand the data. This is known as high dimensionality.\n",
    "Anything which __can't be read and understood without any use of external resources__ is an example of high dimensionality.\n",
    "\n",
    "#### Lost in high dimensional space: ![Imgur](https://i.imgur.com/zPXvzBY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYm_S9AGW58Y"
   },
   "source": [
    "<a id=section103></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajRA6DyGW58b"
   },
   "source": [
    "### 1.3 MOTIVATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3I-1G_fDW58e"
   },
   "source": [
    "When dealing with real problems and real data we often deal with __high dimensional__ data that can go up to __millions__.\n",
    "- Sometime we might need to deal with data having large number of columns/variables, so we need to __reduce its dimensionality__.\n",
    "- The need to reduce dimensionality is often __associated with visualizations__ (reducing to 2â€“3 dimensions so we can plot it) but _that is not always the case_.\n",
    "- Sometimes we might value __performance over precision__ so we could reduce _1,000 dimensional data to 10 dimensions so we can manipulate it faster_ (eg. calculate distances).\n",
    "- Find essential __attributes/variables__.\n",
    "\n",
    "The need to reduce dimensionality at times is __real and has many applications__.\n",
    "For the same, there are __various techinques__.<br/>\n",
    "This sheet is entirely focused on __PCA(Principal Component Analysis)__![Imgur](https://i.imgur.com/XF6Quuj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aACOLL6QW58f"
   },
   "source": [
    "The picture above explains a simple dimension reduction in which a __3-D__ figure is compressed to a __2-D__ figure.\n",
    "This helps in better __visualisation__ and better __understanding__ of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHIm8MzWW58h"
   },
   "source": [
    "<a id=section104></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRk9GWH2W58l"
   },
   "source": [
    "### 1.4 PRINCIPAL COMPONENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMAEuZQ8W58n"
   },
   "source": [
    "> _Too many variables? Should you be using all possible variables to generate model?_\n",
    "\n",
    "In order to handle __'curse of dimensionality'__ and avoid issues like __over-fitting__ in high dimensional space, methods like __Principal Component analysis__ is used.\n",
    "\n",
    "PCA is a method used for __compressing__ a lot of data into something that captures the __essence__ of the _original data_.\n",
    "- It reduces the dimension of your data with the __aim of retaining__ as _much information as possible_. \n",
    "- Calculated efficiently with computer programs\n",
    "- This method combines __highly correlated variables__ together to form a smaller number of an artificial set of variables.<br/>   \n",
    "- These artificial set of variables are called __'principal components'__ that account for __most variance__ in the data.\n",
    "- For much detailed and basic explanation _click on the __video__ just below_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNlJ6_LPW58q"
   },
   "source": [
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=BfTMmoDFXyE\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/BfTMmoDFXyE/0.jpg\" \n",
    "alt=\"A layman's introduction to principal component analysis\" width=\"240\" height=\"180\" border=\"10\" /></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-I0dl7LW58t"
   },
   "source": [
    "This image below is an example for __visualization__, as how _different dimensions are arranged_.\n",
    "As the __dimensionality increase__, the __complexity in visualization increases__.\n",
    "In the image below, we can see that in \n",
    "- _1 dimension we have 10 positions which is easy to read and understand_.\n",
    "- _2 dimensions is having 100 positions, it is still good_.\n",
    "- _3 dimensions is having 1000 posiitons, it is now a bit difficult to read, as we have to check through 3 corners to understand the data well_.\n",
    "\n",
    "__Note__ : Though we can go for __N-Dimensions__ (N=1,2,3,.....,1000,....,N), but __4-D and above__ cannot be drawn on a piece of paper as 1-D, 2-D and 3-D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SlOXANiW58w"
   },
   "source": [
    "![Imgur](https://i.imgur.com/G4RkZPT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3r8vnc4W58z"
   },
   "source": [
    "<a id=section105></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUnCItO7W582"
   },
   "source": [
    "### 1.4.1 PCA IS NOTHING BUT COORDINATE SYSTEM TRANSFORMATION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAMyQwb5W587"
   },
   "source": [
    "The output model uses three axes:<br/> L (Length), W (Width) and H (Height) that perpendicular to each other to represent the 3-D world. So each data point on that object can be written as a function of three variables:\n",
    "\n",
    "    Data(i) = f(L(i), W(i), H(i))         [function 1]\n",
    "\n",
    "In the new coordinate system, each data point on that ellipse can be re-written as a function of two variables:\n",
    "\n",
    "    Data(j) = g(C1(j), C2(j))             [function 2]\n",
    "\n",
    "\n",
    "- Fewer variables (or lower dimensions of variables) of function 2 compared to function 1.\n",
    "        (L, W, H) --> (C1, C2)\n",
    "- No information lost.\n",
    "        function 1 == function 2\n",
    "        The relative geometric positions of all data points remain unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSlNIGvGW58_"
   },
   "source": [
    "![Imgur](https://i.imgur.com/k0rvKf1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kc0ekX_iW59H"
   },
   "source": [
    "<a id=section106></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RjmC7bAW59J"
   },
   "source": [
    "### 1.4.2 PCA has limitations : example of failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGL-oCrEW59M"
   },
   "source": [
    "Any algorithm could __fail__ when its __assumption is not satisfied__. \n",
    "- PCA makes the __\"largest variance\"__ assumptions.\n",
    "- If the data does not follow a multidimensional normal distribution\n",
    "- PCA may not give the best principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZ7GU4BuW59P"
   },
   "source": [
    "![Imgur](https://i.imgur.com/8RS7F6E.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPKZRrrQW59S"
   },
   "source": [
    "<a id=section107></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpu_uB7VW59W"
   },
   "source": [
    "### 1.4.3 PCA as a whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "baPjpJ3fW59a"
   },
   "source": [
    "![Imgur](https://i.imgur.com/LN5YZVm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qk3jfZLEW59d"
   },
   "source": [
    "<a id=section108></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbytbAQEW59f"
   },
   "source": [
    "### 1.4.4 PCA explanation through animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NckbaMp2W59i"
   },
   "source": [
    "PCA will find the __\"best\"__ line according to __two different criteria__ of what is the \"best\".\n",
    "- First, the variation of values along the line should be __maximal__. \n",
    "    - Pay attention to how the __\"spread\" (variance)__ of the _red dots_ changes while the line rotates.\n",
    "    - __can you see when it _reaches maximum_?__ \n",
    "- Second, if we __reconstruct__ the original two characteristics (__position of a blue dot__) from the new one (__position of a red dot__), the __reconstruction error__ will be given by the _length of the connecting red line_.\n",
    "- Observe how the length of these red lines changes while the line rotates.\n",
    "    - __Can you see when the total length _reaches minimum_?__\n",
    "\n",
    "If you stare at this animation for some time,\n",
    "- You will notice that __\"the maximum variance\"__ and __\"the minimum error\"__ are reached at the __same time__, namely when the line points to the magenta ticks I marked on both sides of the data cloud. \n",
    "    - This line corresponds to the _new data property that will be constructed by PCA_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rKbqEcEW59j"
   },
   "source": [
    "![image.png](https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/Q7HIP.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeGFSLO_W59l"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Thus PCA is a method that brings together:\n",
    "1. A measure of how each variable is associated with one another. (Covariance matrix.)\n",
    "2. The directions in which our data are dispersed. (Eigenvectors.)\n",
    "3. The relative importance of these different directions. (Eigenvalues.)\n",
    "\n",
    "__PCA combines our predictors and allows us to drop the eigenvectors that are relatively unimportant__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A square matrix of numbers that describe the **variance of the data, and the covariance among variables** is called covariance matrix. \n",
    "- It is an **empirical description** of data we observe.\n",
    "\n",
    "- For a 2 x 2 matrix, a covariance matrix might look like this:\n",
    "![Imgur](https://i.imgur.com/4rUDI9N.jpg)\n",
    "\n",
    "- The numbers on the upper left and lower right represent the **variance of the x and y** variables.\n",
    "- While the identical numbers on the lower left and upper right represent the **covariance between x and y**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical Representation:\n",
    "- If two variables **increase and decrease together** (a line going up and to the right), they have a **positive covariance**.\n",
    "- If one **decreases while the other increases**, they have a **negative covariance** (a line going down and to the right).\n",
    "![Imgur](https://i.imgur.com/Suk3cPw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us understand the concept of Eigenvectors and Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/0Aj8Ghr.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An **Eigen vector** is a vector whose direction remains unchanged when a linear transformation is applied to it. \n",
    "- The submission of squared distances from origin of all data points is called **Eigen Value**.\n",
    "- The eigenvector with the highest eigenvalue is therefore the **principal component**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0yeQkFcW59m"
   },
   "source": [
    "<a id=section2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tykr4Oj5W59p"
   },
   "source": [
    "<a id=section3></a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8nXtP9f2W5_b",
    "KQKlysTrW5_s",
    "dN9Zx5KMW5_1",
    "EZpB6KMuW5__",
    "b84JtOmcW6Ac",
    "e3EyzKPuW6Ec"
   ],
   "name": "PrincipalComponentAnalysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
